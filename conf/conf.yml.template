---

host_groups:
  group0: [server0]
  group1: [server1]
  group2: [server2]

group_services:
  group0: [AMBARI_SERVER, NAMENODE, ZKFC, HIVE_METASTORE, SPARK_THRIFTSERVER,FLINK_HISTORYSERVER,HISTORYSERVER,RANGER_TAGSYNC, RANGER_USERSYNC,ZOOKEEPER_SERVER,JOURNALNODE]
  group1: [APP_TIMELINE_SERVER, RANGER_ADMIN,NAMENODE, METRICS_GRAFANA,ZKFC, HBASE_MASTER, ZOOKEEPER_SERVER, DATANODE, NODEMANAGER, RESOURCEMANAGER, SPARK_JOBHISTORYSERVER, INFRA_SOLR, JOURNALNODE,KAFKA_BROKER]
  group2: [TIMELINE_READER,YARN_REGISTRY_DNS,METRICS_COLLECTOR, RESOURCEMANAGER, HBASE_REGIONSERVER, ZOOKEEPER_SERVER, DATANODE, NODEMANAGER,HIVE_SERVER, JOURNALNODE,WEBHCAT_SERVER,KAFKA_BROKER]

##################################
## other security configuration ##
##################################
default_password: 'AsdQwe123456'                          # a default password for all required passwords which are not specified in the blueprint,

###########################
## cluster configuration ##
###########################
cluster_name: 'mytestcluster'
hdfs_ha_name: 'mytestcluster'

#plugin 集群安装之前执行一些自定义的操作
plugins:
  - nexus_install_plugin: {enabled: false} #开启的话默认在脚本机器安装 一个nexus 仓库，并且会后续安装会自动配置使用该仓库，使用该功能需要提供一个nexus仓库压缩包

nexus:
  install_dir: /usr/local/
  jdk_install_dir: /opt/jvm/
  user_name: admin            #don't change this
  user_pwd: admin222

#如果机器上没有配置好 centos  ambari 等相关的 repo, 可以在这里配置对应 centos 仓库的地址，脚本会自动添加到所有机器上 不配置就默认使用机器自带的repo
# ambari_repo 必须存在
repos:
   - {name: "ambari_repo", url: "http://server0:8081/repository/yum/sdp_3.1"}
   - {name: "centos_base_repo", url: "http://server0:8081/repository/centos/7/os/x86_64"}
backup_old_repo: no   #是否备份老的yum repo 到 /etc/yum.repos.d/bak，然后删除老的repo 只保留这配置的repo

#额外的需要在所有机器安装的包
packages_need_install: []

#(可默认)ntp时钟同步主机，ntp_server为空则默认为主机第一台，不为空则为指定的ip
external_ntp_server_hostname: ''                                             # if this is empty, ntp server will install and prepare on the ambari-server node

data_dirs: ["/data/sdv1"]


###########################
## general configuration ##
###########################

external_dns: yes                                         # set to yes to use the existing DNS (when no, it will update the /etc/hosts file - must be set to 'no' when using Azure)
disable_firewall: yes                                      # set to yes to disable the existing local firewall service (iptables, firewalld, ufw)
timezone: Asia/Shanghai

registry_dns_bind_port: "53"
blueprint_name: 'my_blueprint'         # the name of the blueprint as it will be stored in Ambari
############################
## database configuration ##
############################
#如果使用了外部数据库，需要手动创建对应的ambari 和 hive 还有ranger的用户，数据库
database: 'postgres'                                      # can be set to  'postgres', 'mysql'
postgres_port: 5432
mysql_port: 3306
database_options:
  repo_url: ''
  external_hostname: ''                                   # if this is empty, Ansible will install and prepare the databases on the ambari-server node
  ambari_db_name: 'ambari'
  ambari_db_username: 'ambari'
  ambari_db_password: 'bigdata'
  hive_db_name: 'hive'
  hive_db_username: 'hive'
  hive_db_password: 'hive'
  rangeradmin_db_name: 'ranger'
  rangeradmin_db_username: 'ranger'
  rangeradmin_db_password: 'ranger'
  rangerkms_db_name: 'rangerkms'
  rangerkms_db_username: 'rangerkms'
  rangerkms_db_password: 'rangerkms'


#####################################
## kerberos security configuration ##
#####################################

security: 'none'                                         # can be set to 'none', 'mit-kdc'
security_options:
  external_hostname: ''                                   # if this is empty, Ansible will install and prepare the MIT KDC on the Ambari node
  external_hostip: ''                                      # used to config /etc/hosts dns look up
  realm: 'MY-REALM.COM'
  admin_principal: 'admin/admin'                          # the Kerberos principal that has the permissions to create new users (don't append the realm)
  admin_password: "{{ default_password }}"
  kdc_master_key: "{{ default_password }}"                # only used when security is set to 'mit-kdc'
  http_authentication: yes                                # set to yes to enable HTTP authentication (SPNEGO)
  manage_krb5_conf: yes                                   # set to no if using FreeIPA/IdM

##########################
## ranger configuration ##                                # only useful if blueprint is dynamic
##########################

ranger_options:                                           # only used if RANGER_ADMIN is part of the blueprint stack
  enable_plugins: no                                     # set to 'yes' if the plugins should be enabled for all of the installed services

ranger_security_options:                                  # only used if RANGER_ADMIN is part of the blueprint stack
  ranger_admin_password: "{{ default_password }}"         # the password for the Ranger admin users (both admin and amb_ranger_admin)
  ranger_keyadmin_password: "{{ default_password }}"      # the password for the Ranger keyadmin user (will only be set in HDP3, in HDP2 it will remain the default keyadmin)
  kms_master_key_password: "{{ default_password }}"       # password used for encrypting the Master Key

##########################
## ambari configuration ##
##########################
# ambari_server
ambari_options:
  ambari_run_user: 'ambari'
  ambari_shell_password: 'ambari'
  ambari_admin_user: 'admin'
  ambari_admin_password: 'admin'
  ambari_admin_default_password: 'admin'                  # no need to change this (unless the Ambari default changes)
  config_recommendation_strategy: 'ALWAYS_APPLY'           # choose between 'NEVER_APPLY', 'ONLY_STACK_DEFAULTS_APPLY', 'ALWAYS_APPLY', 'ALWAYS_APPLY_DONT_OVERRIDE_CUSTOM_VALUES'

wait: true                                                # wait for the cluster to finish installing
wait_timeout: 60                                        # 1 minutes
accept_gpl: yes                                           # set to yes to allow Ambari to install GPL licensed libraries

########################
## path configuration ##
########################
# Common base dirs
base_log_dir: "/var/log"
base_tmp_dir: "/tmp"

# Services base dirs
kafka_log_base_dir: "{% for dr in data_dirs %}{{ dr }}/kafka-logs{% if not loop.last %},{% endif %}{% endfor %}"
ams_base_dir: "/var/lib"
ranger_audit_hdfs_filespool_base_dir: "{{ base_log_dir }}"
ranger_audit_solr_filespool_base_dir: "{{ base_log_dir }}"

# HDFS main dirs
hdfs_dfs_namenode_checkpoint_dir: "{{ hadoop_base_dir }}/hdfs/namesecondary"
hdfs_dfs_namenode_name_dir: "{{ hadoop_base_dir }}/hdfs/namenode"        #one data dir
hdfs_dfs_journalnode_edits_dir: "{{ hadoop_base_dir }}/hdfs/journalnode" #one data dir
hdfs_dfs_datanode_data_dir: "{% for dr in data_dirs %}{{ dr }}/hadoop/hdfs/data{% if not loop.last %},{% endif %}{% endfor %}" #multiple data dir


# YARN main dirs
yarn_nodemanager_local_dirs: "{{ hadoop_base_dir }}/yarn/local"
yarn_nodemanager_log_dirs: "{{ hadoop_base_dir }}/yarn/log"
yarn_timeline_leveldb_dir: "{{ hadoop_base_dir }}/yarn/timeline"


# Other dirs
zookeeper_data_dir: "{{ hadoop_base_dir }}/zookeeper"
infra_solr_datadir: "{{ hadoop_base_dir }}/ambari-infra-solr/data"
heap_dump_location: "{{ base_tmp_dir }}"
hive_downloaded_resources_dir: "{{ base_tmp_dir }}/hive/${hive.session.id}_resources"
heap_dump_location: /tmp

stack_major_version: 3

ansible_tmp_dir: /tmp/ansible
ansible_options:
  ansible_ssh_port: 22



