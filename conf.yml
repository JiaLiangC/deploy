---

host_groups:
  group0: [gs-server0]
  group1: [gs-server2]
  group2: [gs-server3]

group_services:
  group0: [AMBARI_SERVER, NAMENODE,METRICS_COLLECTOR, ZKFC, HIVE_METASTORE, SPARK_THRIFTSERVER,FLINK_HISTORYSERVER,HISTORYSERVER,RANGER_TAGSYNC, RANGER_USERSYNC,ZOOKEEPER_SERVER,JOURNALNODE]
  group1: [RANGER_ADMIN,NAMENODE, METRICS_GRAFANA,ZKFC, HBASE_MASTER, ZOOKEEPER_SERVER, DATANODE, NODEMANAGER, RESOURCEMANAGER, SPARK_JOBHISTORYSERVER, INFRA_SOLR, JOURNALNODE,KAFKA_BROKER]
  group2: [HBASE_REGIONSERVER, ZOOKEEPER_SERVER, DATANODE, NODEMANAGER,HIVE_SERVER, JOURNALNODE,SOLR_SERVER,WEBHCAT_SERVER,KAFKA_BROKER]

###########################
## cluster configuration ##
###########################
ansible_options:
  ansible_ssh_port: 22

nexus_options:
  external_nexus_server_ip: 10.202.60.164                 # if this is empty, nexus server will install and prepare on the ambari-server node
  port: 8081
  user: test
  password: nexus_Test123
  data_dir:                                               # if need install nexus ,set this data dirtory to store nexus data

cluster_name: 'mytestcluster'

#(可默认)ntp时钟同步主机，ntp_server为空则默认为主机第一台，不为空则为指定的ip
external_ntp_server_hostname: ''                                             # if this is empty, ntp server will install and prepare on the ambari-server node

data_dirs: ["/data1", "/data/sdv1"]

###########################
## general configuration ##
###########################

external_dns: yes                                         # set to yes to use the existing DNS (when no, it will update the /etc/hosts file - must be set to 'no' when using Azure)
disable_firewall: no                                      # set to yes to disable the existing local firewall service (iptables, firewalld, ufw)
timezone: UTC

############################
## database configuration ##
############################
database: 'postgres'                                      # can be set to  'postgres', 'mysql' or 'mariadb'
postgres_port: 5432
mysql_port: 3306
database_options:
  repo_url: ''
  external_hostname: ''                                   # if this is empty, Ansible will install and prepare the databases on the ambari-server node
  ambari_db_name: 'ambari'
  ambari_db_username: 'ambari'
  ambari_db_password: 'bigdata'
  hive_db_name: 'hive'
  hive_db_username: 'hive'
  hive_db_password: 'hive'
  rangeradmin_db_name: 'ranger'
  rangeradmin_db_username: 'ranger'
  rangeradmin_db_password: 'ranger'
  rangerkms_db_name: 'rangerkms'
  rangerkms_db_username: 'rangerkms'
  rangerkms_db_password: 'rangerkms'


#####################################
## kerberos security configuration ##                     # useful if blueprint is dynamic, but can also be used to deploy the MIT KDC
#####################################

security: 'mit-kdc'                                          # can be set to 'none', 'mit-kdc', 'ipa' or 'active-directory'
security_options:
  external_hostname: ''                                   # if this is empty, Ansible will install and prepare the MIT KDC on the Ambari node
  external_hostip: ''                                      # used to config /etc/hosts dns look up
  realm: 'EXAMPLE.COM'
  admin_principal: 'admin/admin'                                # the Kerberos principal that has the permissions to create new users (don't append the realm)
  admin_password: "{{ default_password }}"
  kdc_master_key: "{{ default_password }}"                # only used when security is set to 'mit-kdc'
  ldap_url: 'ldaps://ad.example.com:636'                  # only used when security is set to 'active-directory'
  container_dn: 'OU=hadoop,DC=example,DC=com'             # only used when security is set to 'active-directory'
  http_authentication: yes                                # set to yes to enable HTTP authentication (SPNEGO)
  manage_krb5_conf: yes                                   # set to no if using FreeIPA/IdM

##########################
## ranger configuration ##                                # only useful if blueprint is dynamic
##########################

ranger_options:                                           # only used if RANGER_ADMIN is part of the blueprint stack
  enable_plugins: yes                                     # set to 'yes' if the plugins should be enabled for all of the installed services

ranger_security_options:                                  # only used if RANGER_ADMIN is part of the blueprint stack
  ranger_admin_password: "{{ default_password }}"         # the password for the Ranger admin users (both admin and amb_ranger_admin)
  ranger_keyadmin_password: "{{ default_password }}"      # the password for the Ranger keyadmin user (will only be set in HDP3, in HDP2 it will remain the default keyadmin)
  kms_master_key_password: "{{ default_password }}"       # password used for encrypting the Master Key


##################################
## other security configuration ##                        # only useful if blueprint is dynamic
##################################

                       # the password for the Ambari admin user
default_password: 'AsdQwe123456'                          # a default password for all required passwords which are not specified in the blueprint,
                                                          # dependency: NiFi password needs to be at least 12 characters

##########################
## ambari configuration ##
##########################
# ambari_server
ambari_options:
  ambari_run_user: 'ambari'
  ambari_shell_password: 'ambari'
  ambari_repo_name: 'sdp_3.1'
  ambari_admin_user: 'admin'
  ambari_admin_password: 'admin'
  ambari_admin_default_password: 'admin'                    # no need to change this (unless the Ambari default changes)
  config_recommendation_strategy: 'NEVER_APPLY'             # choose between 'NEVER_APPLY', 'ONLY_STACK_DEFAULTS_APPLY', 'ALWAYS_APPLY', 'ALWAYS_APPLY_DONT_OVERRIDE_CUSTOM_VALUES'
  ambari_mysql_conn_jar: '{{repo_base_url}}/repository/tools/bigdata/mysql-connector-java.jar'
  ambari_postgres_conn_jar: '{{repo_base_url}}/repository/tools/bigdata/postgresql-42.2.2.jar'

wait: true                                                # wait for the cluster to finish installing
wait_timeout: 3600                                        # 60 minutes
accept_gpl: yes                                           # set to yes to allow Ambari to install GPL licensed libraries


###########################
## folders configuration ##
###########################

base_log_dir: '/var/log'
base_tmp_dir: '/tmp'


hdfs_ha_name: "sdp3"

#blueprint
blueprint_name: 'udhThree_blueprint'         # the name of the blueprint as it will be stored in Ambari
#blueprint_file: ''                                              # default is empty, if filled, user specified blueprint_file will used to install cluster
#cluster_template_file: 'cluster_template.json'                    # the cluster creation template file

##!!!
#### repo_base_url: this variable is dynamically generated by blueprint_utils.py accroding nexus_options.external_nexus_server_ip , no need to set

jdk_uri: '{{repo_base_url}}/repository/tools/bigdata/jdk.zip'


arthas_url: '{{repo_base_url}}/repository/tools/bigdata/arthas.tar.gz'
perf_tools_url: '{{repo_base_url}}/repository/tools/linux/perf-tools.tar.gz'

#todo nexus 安装后自动设置密码，设置blob 数据存储目录
#http://mirrors.aliyun.com/centos/7/os/x86_64/Packages
openjdk_path: '/usr/local/jdk'    #不可更改
target_krb5_version: 1.15.1-50.el7


#bigdata component configuration

#################################
## dynamic blueprint variables ##
#################################
registry_dns_bind_port: "53"


########################
## path configuration ##
########################
# Common base dirs
base_log_dir: "/var/log"
base_tmp_dir: "/tmp"

# Services base dirs

#hadoop_base_dir: "{{ data_dirs | first  }}/hadoop" !!!  this variable is dynamically generated by blueprint_utils.py

kafka_log_base_dir: "/kafka-logs"
ams_base_dir: "/var/lib"
ranger_audit_hdfs_filespool_base_dir: "{{ base_log_dir }}"
ranger_audit_solr_filespool_base_dir: "{{ base_log_dir }}"

# HDFS main dirs
hdfs_dfs_namenode_checkpoint_dir: "{{ hadoop_base_dir }}/hdfs/namesecondary"
hdfs_dfs_namenode_name_dir: "{{ hadoop_base_dir }}/hdfs/namenode"        #one data dir
hdfs_dfs_journalnode_edits_dir: "{{ hadoop_base_dir }}/hdfs/journalnode" #one data dir
hdfs_dfs_datanode_data_dir: "{% for dr in data_dirs %}{{ dr }}/hadoop/hdfs/data{% if not loop.last %},{% endif %}{% endfor %}" #multiple data dir


# YARN main dirs
yarn_nodemanager_local_dirs: "{{ hadoop_base_dir }}/yarn/local"
yarn_nodemanager_log_dirs: "{{ hadoop_base_dir }}/yarn/log"
yarn_timeline_leveldb_dir: "{{ hadoop_base_dir }}/yarn/timeline"


# Other dirs
zookeeper_data_dir: "{{ hadoop_base_dir }}/zookeeper"
infra_solr_datadir: "{{ hadoop_base_dir }}/ambari-infra-solr/data"
heap_dump_location: "{{ base_tmp_dir }}"
hive_downloaded_resources_dir: "{{ base_tmp_dir }}/hive/${hive.session.id}_resources"
heap_dump_location: /tmp

stack_major_version: 3

ansible_tmp_dir: /tmp/ansible
centos_repo_url: "{{repo_base_url}}/repository/centos/7/os/x86_64"

